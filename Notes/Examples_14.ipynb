{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-amplifier",
   "metadata": {},
   "source": [
    "# MATH 3375 Examples Notebook #14\n",
    "\n",
    "# k-Nearest Neighbors (kNN)\n",
    "\n",
    "We explore another algorithm for predicting classification (categorical response), using the **mtcars** and **iris** data sets.\n",
    "\n",
    "Again, our response variable is **am**, the transmission type, where 0=automatic, and 1=manual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at data set\n",
    "head(mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-establishment",
   "metadata": {},
   "source": [
    "## Another Way to Explore Potential Predictors \n",
    "\n",
    "Below, we visualize 2 predictors of transmission type in a 2-dimensional plot, color-coding the points by transmission type. Our 2 predictors are displacement (**disp**) and rear axle ratio (**drat**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = c(16,17)\n",
    "colors = c(\"green\", \"red\")\n",
    "plot(disp ~ drat, main=\"Displacement and Rear Axle Ratio by Transmission Type\", xlab=\"Rear Axle Ratio\", ylab=\"Displacement\",\n",
    "    col=colors[factor(mtcars$am)], pch=shapes[factor(mtcars$am)], data=mtcars)\n",
    "\n",
    "legend(\"topright\",\n",
    "       legend = c(\"Automatic\",\"Manual\"),\n",
    "       pch = shapes,\n",
    "       col = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-thickness",
   "metadata": {},
   "source": [
    "#### Predicting Transmission Type by _Proximity_ to Other Points\n",
    "\n",
    "We can see that these 2 variables often do a good job of visually separating manual from automatic transmissions on the plot. \n",
    "\n",
    "Below we generate the same plot with a **_NEW_** point for which we want to predict the transmission type.  It makes sense to predict that the transmission type will match the closest surrounding points, which are automatic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(disp ~ drat, main=\"Displacement and Rear Axle Ratio by Transmission Type\", xlab=\"Rear Axle Ratio\", ylab=\"Displacement\",\n",
    "    col=colors[factor(mtcars$am)], pch=shapes[factor(mtcars$am)], data=mtcars)\n",
    "\n",
    "legend(\"topright\",\n",
    "       legend = c(\"Automatic\",\"Manual\"),\n",
    "       pch = shapes,\n",
    "       col = colors)\n",
    "points(3.2,240,pch=15,cex=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-advancement",
   "metadata": {},
   "source": [
    "#### Examine Points Nearby \n",
    "\n",
    "The point closest to the new \"unknown\" point is an automatic transmission.  In fact, we can see that the 3 closest points all represent cars with automatic transmissions. If we expand the \"neighborhood\" we can see that 4 of the 5 closest points are automatic transmission.\n",
    "\n",
    "The three observations above illustrate the strategy of \"**_k-Nearest Neighbors_**\".\n",
    "\n",
    "* When k=1, we examine only one point (the one closest to the new unknown point).\n",
    "* When k=3, we examine the closest 3 points.\n",
    "* When k=5, we examine the closest 5 points.\n",
    "\n",
    "In each case above, most (or all) of the nearest neighbors have automatic transmission, so we would predict that this new point represents a vehicle with automatic transmission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-turning",
   "metadata": {},
   "source": [
    "### What Value Should We Use for k?\n",
    "\n",
    "* We can see how k=1 might not be sufficient. Consider the case below, where the closest point is a manual transmission, but that one point is somewhat unusual, and there are many more automatic transmission points in the vicinity.\n",
    "* The optimal value for k will vary depending on the data set.\n",
    "* By letting k be odd, we can be assured of a majority \"vote\" from the nearest neighbors of the unknown point.\n",
    "\n",
    "One way to choose a k value is to try several different kNN models with different k values on a test set, and determine which value of k results in the best prediction performance.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(disp ~ drat, main=\"Displacement and Rear Axle Ratio by Transmission Type\", xlab=\"Rear Axle Ratio\", ylab=\"Displacement\",\n",
    "    col=colors[factor(mtcars$am)], pch=shapes[factor(mtcars$am)], data=mtcars)\n",
    "\n",
    "legend(\"topright\",\n",
    "       legend = c(\"Automatic\",\"Manual\"),\n",
    "       pch = shapes,\n",
    "       col = colors)\n",
    "points(3.4,300,pch=15,cex=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-usage",
   "metadata": {},
   "source": [
    "### How is \"Nearest\" Determined?\n",
    "\n",
    "There are many ways to measure \"distance\" between points. One of the most popular is the Euclidean distance formula: For a new point $\\left(x_{new},y_{new}\\right)$ and an existing observation $j$ with coordinates $\\left(x_j,y_j\\right)$, the distance between the points is computed as:\n",
    "\n",
    "$$d = \\sqrt{\\left(x_{new} - x_j \\right)^2 + \\left(y_{new} - y_j \\right)^2}$$\n",
    "\n",
    "This is a well-known distance formula for points in 2 dimensions (like the example above). The same principle can work for data points in any number of dimensions.  In general, for data points with $p$ predictors, the distance between points $a$ and $b$ is :\n",
    "\n",
    "$$d = \\sqrt{\\sum_{i=1}^p{\\left(a_i - b_i \\right)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-bridal",
   "metadata": {},
   "source": [
    "## More than 2 Categories\n",
    "\n",
    "kNN is one of several \"classification\" algorithms that can predict values for response variables with _more than just 2 values_. Below we see two plots of the Iris data set using 2 dimensions to show a pair of predictors, with color to show the Species in each plot. There are 3 possible species, and k-Nearest Neighbors can be used to predict the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = c(15,16,17)\n",
    "colors = c(\"red\", \"blue\", \"mediumorchid\")\n",
    "plot(iris$Petal.Length,iris$Petal.Width, main=\"Iris Dimensions by Species\", xlab=\"Petal Length\", ylab=\"Petal Width\",\n",
    "    col=colors[factor(iris$Species)], pch=shapes[factor(iris$Species)])\n",
    "\n",
    "legend(\"topleft\",\n",
    "       legend = levels(iris$Species),\n",
    "       pch = shapes,\n",
    "       col = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = c(15,16,17)\n",
    "colors = c(\"red\", \"blue\", \"mediumorchid\")\n",
    "plot(iris$Petal.Length,iris$Sepal.Width, main=\"Iris Dimensions by Species\", xlab=\"Petal Length\", ylab=\"Sepal Width\",\n",
    "    col=colors[factor(iris$Species)], pch=shapes[factor(iris$Species)])\n",
    "\n",
    "legend(\"topright\",\n",
    "       legend = levels(iris$Species),\n",
    "       pch = shapes,\n",
    "       col = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-syndrome",
   "metadata": {},
   "source": [
    "### Which Predictors to Use\n",
    "\n",
    "Notice that we no longer need for our plot to have a linear appearance for the points to be reasonably well differentiated between species. There are a few reasons for this:\n",
    "\n",
    "* The dimensions of the plot are ALL predictors; the response variable is only shown by the color/shape of the points.\n",
    "* Although the plot shows how the predictors may be related, that is not why the plot is useful; we are not trying to determine the relationship between the predictors; we want to know how the predictors are related to the response variable.\n",
    "* The plot is being used to arrange the points in a space where similar points are likely to be closer to each other; this does not require a linear relationship between the predictors OR between any predictor and the response variable.\n",
    "* The KNN model does not compute correlation (which indicates strength of a linear relationship).  The KNN model only computes **_distance_** to neighboring points.\n",
    "\n",
    "This helps explain why the second graph above is about as useful as the first in differentiating points by species.\n",
    "\n",
    "However, this does not mean that any set of predictors will be as good as any other. Notice the plot below, which uses Sepal Length and Sepal Width as its 2 predictors. The Virginica and Versicolor species are not well differentiated at all. This suggests that using these 2 predictors for KNN would not give us a very good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = c(15,16,17)\n",
    "colors = c(\"red\", \"blue\", \"mediumorchid\")\n",
    "plot(iris$Sepal.Length,iris$Sepal.Width, main=\"Iris Dimensions by Species\", xlab=\"Sepal Length\", ylab=\"Sepal Width\",\n",
    "    col=colors[factor(iris$Species)], pch=shapes[factor(iris$Species)])\n",
    "\n",
    "legend(\"topright\",\n",
    "       legend = levels(iris$Species),\n",
    "       pch = shapes,\n",
    "       col = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-combining",
   "metadata": {},
   "source": [
    "#### Using Boxplots to Select Good Predictors\n",
    "\n",
    "The best _individual_ predictors will be the ones that vary the most from one classification to another. We can use boxplots to see which predictors have the most potential for effectively separating the different 'classes'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(2,2))\n",
    "boxplot(Petal.Length~Species,data=iris,main=\"Petal Length\")\n",
    "boxplot(Petal.Width~Species,data=iris,main=\"Petal Width\")\n",
    "boxplot(Sepal.Length~Species,data=iris,main=\"Sepal Length\")\n",
    "boxplot(Sepal.Width~Species,data=iris,main=\"Sepal Width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-charter",
   "metadata": {},
   "source": [
    "#### Differentiating Species by Each Predictor\n",
    "\n",
    "Above, we can see that Sepal Width does not vary enough between Virginica and Versicolor to be a very effective predictor. Possibly, pairing Sepal Width with another variable would provide better separation, but pairing it with Sepal Length was not effective, as the previous plot shows.  \n",
    "\n",
    "Note that when Sepal Length was paired with Petal Length, the points were well separated because of their difference in Petal Length. The Sepal Length dimension did not play near as big of a role in differentiating the points.\n",
    "\n",
    "When no single predictor provides good separation of data points, combining several features can improve the differentiation. Unlike regression, there is no danger introduced by multicollinearity among the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-short",
   "metadata": {},
   "source": [
    "## One Implementation of k-Nearest Neighbors in R\n",
    "\n",
    "There are multiple libraries that have implementations of k-Nearest Neighbors. Below we use the **knn** function in the **class** library. (Class refers to _classification_, which is the type of model we are using.)\n",
    "\n",
    "Note that we split the data set into training and test data. We use the training data set as the reference point for finding the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"class\")\n",
    "library(class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train/test sets\n",
    "set.seed(3375)\n",
    "\n",
    "head(iris)\n",
    "\n",
    "train_rows <- sample(1:nrow(iris),120)\n",
    "iris_train <- iris[train_rows,]\n",
    "iris_test <- iris[-train_rows,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Sepal Length and Width as Predictors (k=3)\n",
    "iris_knn3_a <- knn(iris_train[,1:2], iris_test[,1:2], cl=iris_train[,5], k = 3)\n",
    "data.frame(iris_knn3_a,iris_test$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Sepal Length and Width as Predictors (k=7)\n",
    "iris_knn7_a <- knn(iris_train[,1:2], iris_test[,1:2], cl=iris_train[,5], k = 7)\n",
    "data.frame(iris_knn7_a,iris_test$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris_knn3_b <- knn(iris_train[,c(1,3)], iris_test[,c(1,3)], cl=iris_train[,5], k = 3)\n",
    "data.frame(iris_knn3_b,iris_test$Species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-academy",
   "metadata": {},
   "source": [
    "### Observations About the Approach\n",
    "\n",
    "* The above examples were kNN models with different values of k (k=3 for first and third, and k=7 for second).\n",
    "* The model is created by identifying the training data AND the test data, as well as the response variable (the 'class').\n",
    "* The result of running the model is a vector of the predicted classes.\n",
    "* There is no \"model summary\" because there are no coefficients or other parameters. Therefore, we call this a **_non-parametric_** model.\n",
    "* Because this is a non-parametric model: \n",
    "   * There are no underlying assumptions about distribution\n",
    "   * There is no need to check diagnostic plots for goodness of model fit\n",
    "   * There is also no \"interpretation\" of the model as there is for regression (e.g., interpretation of coefficients, slope)\n",
    "   * The model is not unduly influenced by outliers\n",
    "\n",
    "### Observations About the Results\n",
    "\n",
    "* As we expected from the earlier plots, Sepal Length and Sepal Width did not differentiate well between Virginica and Versicolor. Therefore, the first model using these predictors with k=3 had many mis-classifications.\n",
    "* Increasing k to 7 with the Sepal Length and Width model reduced the mis-classifications, but did not eliminate them.\n",
    "* Using k=3 with Petal Length and Sepal Length as the only predictors gave 100% accurate predictions on this test set.\n",
    "\n",
    "To improve models that don't have best possible performance, we can try one or more of the following:\n",
    "* Include more features as predictors (possibly all of them)\n",
    "* Change the value of k\n",
    "* Use cross-validation techniques with multiple test/train splits to identify the approach with the best **_average_** performance.\n",
    "\n",
    "When comparing models, we can compute the error rate as $\\frac{incorrect}{total}$, where both numerator and denominator are simple counts. For instance, the kNN-3 with Sepal Length and Width as predictors had 7 incorrect predictions out of the 30 that were made, giving it a $\\frac{7}{30} \\approx 0.23$ error rate.  The model with the lowest error rate is selected as the one with best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-connectivity",
   "metadata": {},
   "source": [
    "## Suggestion\n",
    "\n",
    "Try making other kNN models with the **mtcars** data set to predict transmission type. \n",
    "\n",
    "* Which variables are the most helpful? \n",
    "* Which values of k give the best performance? \n",
    "* Which model gives better predictions-- Logistic Regression or kNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-nickname",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
