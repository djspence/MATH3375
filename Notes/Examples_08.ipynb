{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-amplifier",
   "metadata": {},
   "source": [
    "# MATH 3375 Examples Notebook #8\n",
    "\n",
    "# Validation and Cross-Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-contamination",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "We will demonstrate validation and cross-validation with the housing data set. We are creating a model to predict the price of the house using square footage, number of full baths, and bike score as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at data set\n",
    "houses <- read.csv(\"HousingBrief.csv\")\n",
    "head(houses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-perspective",
   "metadata": {},
   "source": [
    "## Partitioning the Data\n",
    "\n",
    "First we will demonstrate a simple partition for training and testing. We will use a random 70% of the data for training and save the rest for testing. \n",
    "\n",
    "To make the process as transparent as possible, we start by determining the number of data points (rows) in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow(houses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-auckland",
   "metadata": {},
   "source": [
    "#### Each row has an index, from 1 to 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_idx <- 1:nrow(houses)\n",
    "houses_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-giant",
   "metadata": {},
   "source": [
    "### Randomly Sample the Row Numbers\n",
    "\n",
    "We can use R's **sample** command to randomly select 70% of the rows.  For this example, that is 73 rows.\n",
    "\n",
    "Note that when running a random process (like a sample) it is best practice to start with the **set.seed** command. This ensures that the code will produce the same results every time you run it. We want this to be the case to ensure **_reproducibility_**. In the examples for this class, I will use the course number (3375), but any number could be used as the \"seed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3375)\n",
    "\n",
    "train_rows <- sort(sample(houses_idx, 73))\n",
    "train_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-organ",
   "metadata": {},
   "source": [
    "#### The remaining rows (NOT selected as training rows) are the test rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows <- houses_idx[!(houses_idx %in% train_rows)]\n",
    "test_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-directory",
   "metadata": {},
   "source": [
    "### Use the Selected Row Numbers to Partition the Data into Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  <- houses[train_rows, ]\n",
    "\n",
    "nrow(train_data)\n",
    "head(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data   <- houses[test_rows, ]\n",
    "\n",
    "nrow(test_data)\n",
    "head(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-frame",
   "metadata": {},
   "source": [
    "#### Other Methods\n",
    "\n",
    "R also has machine learning libraries (e.g., **caTools**, **dplyr**) that will create train/test splits of a data set. \n",
    "\n",
    "Below is another popular method that uses base R (no other package needed). It is simpler to code, but slightly less precise; the size of the training set is not certain, because the rows are given a 70% probability of being chosen, rather than designating a specific _number_ of rows as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3375)\n",
    "\n",
    "sample <- sample(c(TRUE, FALSE), nrow(houses), replace=TRUE, prob=c(0.7,0.3))\n",
    "train_data_alt  <- houses[sample, ]\n",
    "test_data_alt   <- houses[!sample, ]\n",
    "\n",
    "nrow(train_data_alt)\n",
    "nrow(test_data_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-fundamentals",
   "metadata": {},
   "source": [
    "## Train Model with Training Set\n",
    "\n",
    "We will proceed with our original training and test set.  Below we create a model using only the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_model <- lm(price2014 ~ squarefeet + full_baths + bikescore, data=train_data)\n",
    "summary(price_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-smart",
   "metadata": {},
   "source": [
    "### In-Sample Error\n",
    "\n",
    "We'll compute the Mean Square Error (MSE) for the data points in the training set.  This is the **_in-sample_** error.\n",
    "\n",
    "Notice that we can take advantage of the fact that R has already calculated the residuals ($y - \\widehat{y}$) for every point in the training set.  We can get these values using the **resid** function.  \n",
    "\n",
    "The MSE is simply the average of the squared residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- nrow(train_data)\n",
    "residuals <- resid(price_model)\n",
    "MSE_train <- sum(residuals^2)/n\n",
    "\n",
    "MSE_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-fantasy",
   "metadata": {},
   "source": [
    "### Out-Of-Sample Error \n",
    "\n",
    "We should be more interested in the MSE for the test set, since it was NOT used to train the model.  This is the **_out-of-sample_** error. To get the residuals for those data points, we need to do the following:\n",
    "\n",
    "1. Compute the model predictions\n",
    "2. Compute the residual for each data point arithmetically ($y - \\widehat{y}$)\n",
    "3. Compute the mean of the squared residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds <- predict(price_model,test_data)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_resids <- test_data$price2014 - test_preds\n",
    "test_resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = nrow(test_data)\n",
    "test_MSE = sum(test_resids^2)/n\n",
    "test_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-google",
   "metadata": {},
   "source": [
    "### Compare In-Sample and Out-of-Sample Error\n",
    "\n",
    "As the data show, the in-sample error is much lower. This is to be expected, since the model was trained on the sample.  The out-of-sample error gives a better estimate of the error we could expect when using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-football",
   "metadata": {},
   "source": [
    "### Cross-Validation \n",
    "\n",
    "The **caret** package is one of the most popular R packages for performing cross-validation. However, it cannot be installed in this JupyterHub environment. An example is shown below of the code that would be used to conduct 10-fold cross-validation with the **caret** library.  \n",
    "\n",
    "    set.seed(3375)\n",
    "    train_control <- trainControl(method = \"cv\", number = 10)\n",
    " \n",
    "    model <- train(price2014 ~ squarefeet+full_baths+bikescore, data=train_data, method=\"lm\", trControl=train_control)\n",
    " \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-joining",
   "metadata": {},
   "source": [
    "When the model is printed, the metrics are the _average_ out-of-sample errors computed across the several different samples. The final model is the linear model computed with all training data (none held out for validation).  \n",
    "\n",
    "Shown below is a sample of what the output might look like.  Notice that the metric RMSE is the **_root_** mean square error, which is $\\sqrt{MSE}$.  \n",
    "\n",
    "Also note that the $R^2$ is not the same value that would be shown in the model summary (which is based on the training data.) This metric is computed based on how well the model explains the **_test_** data. \n",
    "\n",
    "    Linear Regression \n",
    "\n",
    "    200 samples\n",
    "      3 predictor\n",
    "\n",
    "    No pre-processing\n",
    "    Resampling: Cross-Validated (10 fold) \n",
    "    Summary of sample sizes: 91, 90, 90, 91, 90, 90, ... \n",
    "    Resampling results:\n",
    "\n",
    "      RMSE      Rsquared   MAE     \n",
    "      78.7401   0.6432     53.1476"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
