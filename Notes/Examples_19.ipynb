{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-amplifier",
   "metadata": {},
   "source": [
    "# MATH 3375 Examples Notebook #19\n",
    "\n",
    "# Bootstrap Samples and Ensemble Methods: Bagging and Random Forests\n",
    "\n",
    "**_Ensemble methods_** take the average prediction of multiple models:\n",
    "* For continuous (quantitative) response variable: mean or weighted mean of predicted value from all models\n",
    "* For classification: majority \"vote\" of the the predicted class from all models\n",
    "\n",
    "This has the effect of:\n",
    "* Reducing variance\n",
    "* Reducing the effect of any one model being overfit \n",
    "* Improving prediction overall\n",
    "\n",
    "We will illustrate with the **iris** data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at data set\n",
    "head(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-understanding",
   "metadata": {},
   "source": [
    "## Generating More Samples for Training: Bootstrap Samples\n",
    "\n",
    "To create multiple models, we need more than one training data set. Rather than subdivide the one data set we have into smaller samples, we can leverage the power of **bootstrapping** to create many samples that are just as large as our full original data set. Each 'bootstrap sample' is created by sampling _**with replacement**_ from the original data set. It represents what _another_ sample from the same population **_could_** look like.\n",
    "\n",
    "An example is given below. The iris data set has 150 records.  We create a list of row numbers (**idx**) to create another set of 150, but it is not identical to the original, because some rows may be chosen multiple times, while others may not be chosen at all. The ability to choose rows more than once is the result of sampling with replacement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-investing",
   "metadata": {},
   "source": [
    "### Sample Row Numbers with Replacement\n",
    "\n",
    "The first step is to sample the possible row numbers with replacement to identify which rows will be included in the bootstrap sample. (Note that the row numbers are NOT selected in order, but they are displayed in order to make it easier to see which row numbers were included multiple times, and which were not included at all.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose rows for bootstrap sample\n",
    "set.seed(3375)\n",
    "idx <- sample(1:nrow(iris),nrow(iris),replace=TRUE)\n",
    "sort(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-award",
   "metadata": {},
   "source": [
    "### Use Row Numbers to Create Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot1 <- iris[idx,]\n",
    "head(boot1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-earth",
   "metadata": {},
   "source": [
    "### Compare Bootstrap Sample to Original Data Set \n",
    "\n",
    "The comparison below further illustrates that the bootstrap sample is the same size as our original data set, but has slightly different composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of data sets\n",
    "nrow(iris)\n",
    "nrow(boot1)\n",
    "head(boot1)\n",
    "summary(boot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of data set variables\n",
    "summary(iris$Species)\n",
    "summary(boot1$Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(iris$Petal.Length)\n",
    "summary(boot1$Petal.Length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-tumor",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Bagging (**B**ootstrap **agg**regat**ing**) is a process in which several bootstrap samples are created, and a model (such as a tree) is created for each sample. Then the average prediction from all models combined is used for each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-shannon",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "A random forest uses the principle of bagging with decision trees, but also _**randomly selects which features to use as predictors**_ for each tree. Thus, each tree has both a different training set and a different feature set of predictors.\n",
    "\n",
    "We will implement random forests with the **randomForest** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"randomForest\")\n",
    "library(randomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test and train set\n",
    "\n",
    "test_rows <- c(14,23,80,119,123)\n",
    "iris_test <- iris[test_rows,]\n",
    "iris_train <- iris[-test_rows,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model to predict species\n",
    "iris_model_forest_01 <- randomForest(Species~.,data=iris_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display some information available from model \n",
    "#Variable Importance (Average across all trees) \n",
    "\n",
    "importance(iris_model_forest_01)\n",
    "varImpPlot(iris_model_forest_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "iris_model_forest_02 <- randomForest(Petal.Length~.,data=iris_train)\n",
    "\n",
    "#Variable importance (continuous response variable)\n",
    "\n",
    "importance(iris_model_forest_02)\n",
    "varImpPlot(iris_model_forest_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-deadline",
   "metadata": {},
   "source": [
    "## Using the Random Forest Models for Prediction\n",
    "\n",
    "Using the test set that we set aside, we will see how each random forest can be used for prediction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_species <- predict(iris_model_forest_01,iris_test)\n",
    "test_pred_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_length <- predict(iris_model_forest_02,iris_test)\n",
    "test_pred_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-aquatic",
   "metadata": {},
   "source": [
    "### Comparing Predictions with Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(Actual=iris_test$Species,Predicted=test_pred_species)\n",
    "data.frame(Actual=iris_test$Petal.Length,Predicted=test_pred_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-editing",
   "metadata": {},
   "source": [
    "## Parameters for Fine Tuning Random Forests\n",
    "\n",
    "Just as decision trees can be fine tuned (e.g., pruned to desired level), so can random forests. Some of the most important tuning parameters are:\n",
    "\n",
    "* ntree = number of tress to grow, and the default is 500. \n",
    "* mtry = number of variables randomly sampled as candidates at each split. \n",
    "  The default is sqrt(p) for classfication and p/3 for regression\n",
    "* nodesize = minimum size of terminal nodes. \n",
    "  The default value is 1 for classification and 5 for regression\n",
    "  \n",
    "Ideally, you can try several different values of each parameter to see what yields the best results.\n",
    "\n",
    "The documentation for randomForest gives more detail on the options (parameters) and on what is stored in the model that is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "?randomForest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
