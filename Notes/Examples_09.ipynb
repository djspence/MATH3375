{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-amplifier",
   "metadata": {},
   "source": [
    "# MATH 3375 Examples Notebook #9\n",
    "\n",
    "# Dimension Reduction and Regularization\n",
    "\n",
    "## LASSO and Ridge Regression\n",
    "\n",
    "We continue looking at methods for improving a linear regression model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at data set\n",
    "car_data <- read.csv(\"cars2004.csv\", stringsAsFactors=TRUE)\n",
    "head(car_data,3)\n",
    "tail(car_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data$Length <- as.integer(as.character(car_data$Length))\n",
    "car_data$Width <- as.integer(as.character(car_data$Width))\n",
    "tail(car_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data$Length[is.na(car_data$Length)] <- as.integer(median(car_data$Length[!is.na(car_data$Length)]))\n",
    "car_data$Width[is.na(car_data$Width)] <- as.integer(median(car_data$Width[!is.na(car_data$Width)]))\n",
    "head(car_data,3)\n",
    "tail(car_data,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-courtesy",
   "metadata": {},
   "source": [
    "## 1. Stepwise Regression\n",
    "\n",
    "Recall that we used stepwise regression as a method for selecting variables to include in the model. We review this method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_model0 <- lm(HP ~ 1, data=car_data)\n",
    "\n",
    "hp_model_full <- lm(HP ~ MSRP+Invoice+EngineSize+Cylinders+City.MPG+Hwy.MPG+Weight+WheelBase+Length+Width, data=car_data)\n",
    "hp_model_step = step(hp_model0, scope=list(lower=hp_model0, upper=hp_model_full), direction = \"forward\", k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(hp_model_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-excess",
   "metadata": {},
   "source": [
    "### Pros and Cons of Stepwise \n",
    "\n",
    "#### Pros\n",
    "* Easy and quick\n",
    "* Algorithm helps balance bias and variance\n",
    "* Algorithm uses a metric (e.g., AIC) to avoid adding variables that do not improve this balance\n",
    "\n",
    "#### Cons\n",
    "* Greedy algorithm does not necessarily choose the **_optimal_** model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-moses",
   "metadata": {},
   "source": [
    "## 2. LASSO Regression\n",
    "\n",
    "LASSO (**L**east **A**bsolute **S**hrinkage and **S**election **O**perator) is a regression _regularization_ method with the following properties:\n",
    "\n",
    "* Instead of purely minimizing the SSE (Sum of Squared Residuals) to determine the model coefficients, the algorithm minimizes:\n",
    "<center>\n",
    "$SSE + \\lambda\\sum_{j=1}^{p}\\left | \\beta_j \\right |$\n",
    "<\\center>\n",
    "    \n",
    "* The sum of the coefficient magnitudes is limited \n",
    "* The effect is to _**shrink**_ coefficients\n",
    "* This drives some coefficients to zero, which eliminates the variable from the model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-trinity",
   "metadata": {},
   "source": [
    "#### Points of Caution\n",
    "\n",
    "* The value of $\\lambda$ must be determined through trials with cross-validation to see which $\\lambda$ value is most effective.\n",
    "* Note that when $\\lambda = 0$, the model is the same as an ordinary least squares (OLS) regression model.\n",
    "* Unlike ordinary least squares (OLS) regression, there is no \"closed form\" mathematical equation to compute model coefficients\n",
    "* A _numerical algorithm_ is applied to minimize the sum\n",
    "* Data MUST be standardized for LASSO algorithm to be applied\n",
    "* After LASSO has been used to **_select_** the variables, use those variables again in an OLS (not LASSO) model to obtain the final regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-bracelet",
   "metadata": {},
   "source": [
    "### R Libraries to Implement LASSO\n",
    "\n",
    "There are multiple libraries that will implement LASSO regression. Below we show an implementation with the **glmnet** library.\n",
    "\n",
    "_Only un-comment the install line if the library fails to load. Run the install once and re-comment the line to avoid running the install again._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"glmnet\")\n",
    "library(glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-scholar",
   "metadata": {},
   "source": [
    "#### Features of glmnet\n",
    "\n",
    "* **glmnet** scales the data for you by default\n",
    "* **glmnet** will use cross-validation for you to select the best $\\lambda$ value\n",
    "\n",
    "#### Notes about the R code below\n",
    "\n",
    "* The first step is to find the best $\\lambda$\n",
    "* Because there is cross-validation involved, note that we use **set.seed** before starting to ensure reproducible results.\n",
    "* We are using 5-fold cross-validation by specifying **nfolds=5**\n",
    "* We specify **alpha=1** to indicate that we are using **LASSO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3375)\n",
    "hp_model_lasso <- cv.glmnet(x=as.matrix(car_data[c(4:7,9:14)]),y=as.matrix(car_data[8]),alpha=1,nfolds=5)\n",
    "plot(hp_model_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-triple",
   "metadata": {},
   "source": [
    "### Interpreting the Graph\n",
    "\n",
    "* Each red dot is the MSE obtained for a certain $\\lambda$ value\n",
    "* The vertical line on the LEFT at $log(\\lambda)$ close to $-3$ represents the minimum MSE\n",
    "* Both vertical lines together give an interval where we can remain within one _standard error_ of that minimum\n",
    "* Below we display the lambda values for these two thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lambda with minimum MSE\n",
    "hp_model_lasso$lambda.min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lambda threshold for MSE within 1 SE of minimum\n",
    "hp_model_lasso$lambda.1se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-bruce",
   "metadata": {},
   "source": [
    "### Viewing Model Coefficients\n",
    "\n",
    "Below we view the LASSO model coefficients for the $\\lambda$ with minimum MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(hp_model_lasso,s=hp_model_lasso$lambda.min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-legislature",
   "metadata": {},
   "source": [
    "#### Reduce Dimensions (Number of Predictors)\n",
    "\n",
    "In this example, no coefficients were eliminated from the model, but the **Invoice** coefficient is very small.\n",
    "\n",
    "Now we view the LASSO model coefficients for the largest possible $\\lambda$ that still gives results within 1 SE of the minimum MSE. This is how LASSO can be used to reduce the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(hp_model_lasso,s=hp_model_lasso$lambda.1se)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-permit",
   "metadata": {},
   "source": [
    "#### This is the 'default' LASSO model, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(hp_model_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-horse",
   "metadata": {},
   "source": [
    "#### Using the LASSO Result\n",
    "\n",
    "The default LASSO model with the most shrinkage (largest acceptable $\\lambda$) has dropped the coefficients for all coefficients except MSRP, EngineSize, Cylinders, and City.MPG.\n",
    "\n",
    "To use these results, we create an OLS model with the predictors that were not dropped by the LASSO model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_model_ols_lasso <- lm(HP ~ MSRP+EngineSize+Cylinders+City.MPG, data=car_data)\n",
    "summary(hp_model_ols_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-letter",
   "metadata": {},
   "source": [
    "## 3. Ridge Regression\n",
    "\n",
    "Another regularization method for regression models is **Ridge Regression**. It is _**not**_ used for variable selection, but it does have the potential to better balance bias and variance to give a model with better predictions.\n",
    "\n",
    "Ridge Regression computes model coefficients that minimize:\n",
    "\n",
    "<center>\n",
    "$SSE + \\lambda\\sum_{j=1}^{p} {\\beta_j}^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-fleece",
   "metadata": {},
   "source": [
    "### Using glmnet for Ridge Regression\n",
    "\n",
    "The process is very similar to the one we used for LASSO, but **alpha=0** indicates we are using Ridge Regression instead of LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(3375)\n",
    "hp_model_ridge <- cv.glmnet(x=as.matrix(car_data[c(4:7,9:14)]),y=as.matrix(car_data[8]),alpha=0,nfolds=5)\n",
    "plot(hp_model_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-canon",
   "metadata": {},
   "source": [
    "### Interpreting the Plot\n",
    "\n",
    "Again, we can examine the $\\lambda$ with minimum MSE and the largest acceptable $\\lambda$ to stay within 1 standard error of that minimum MSE.  Notice again the the model with the larger lambda is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(hp_model_ridge, s=hp_model_ridge$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(hp_model_ridge, s=hp_model_ridge$lambda.1se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(hp_model_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-reform",
   "metadata": {},
   "source": [
    "### Notes About the Ridge Model\n",
    "\n",
    "Unlike LASSO, we keep our selected Ridge Model coefficients as our model.  The predictions are computed using these coefficients rather than those in the OLS model with the same predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict HP for first 5 rows of original data set\n",
    "predict(hp_model_ridge, s=\"lambda.min\", newx=as.matrix(car_data[1:5,c(4:7,9:14)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict HP for first 5 rows of original data set\n",
    "predict(hp_model_ridge, s=\"lambda.1se\", newx=as.matrix(car_data[1:5,c(4:7,9:14)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-favorite",
   "metadata": {},
   "source": [
    "### Considerations for Using Ridge Regression\n",
    "\n",
    "Since Ridge does not eliminate variables that are highly correlated It is advisable to select a reasonable subset of variables **first** (to avoid multicollinearity) and then apply Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-description",
   "metadata": {},
   "source": [
    "### Suggestion\n",
    "\n",
    "Compare the Ridge regression predictions with those of the other models (stepwise and OLS model from LASSO dimension reduction). Compute the in-sample MSE for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-instrument",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
