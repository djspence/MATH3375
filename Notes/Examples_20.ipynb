{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-amplifier",
   "metadata": {},
   "source": [
    "# MATH 3375 Examples Notebook #20\n",
    "\n",
    "# Another Ensemble Method: Boosting\n",
    "\n",
    "Whereas **_bagging_** uses multiple independent models in parallel, **_boosting_** iteratively constructs the model from several weaker models by assigning weights at each iteration, depending on how each weaker model performed. \n",
    "\n",
    "This has the effect of:\n",
    "* Reducing bias\n",
    "* Improving (reducing) the training error \n",
    "\n",
    "However, the above effects also mean that the method can lead to overfitting.  This should be controlled with effective **_parameter tuning_** and **_cross-validation_**.\n",
    "\n",
    "We will illustrate with the **spam_base** data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at data set\n",
    "spam_data <- read.csv(\"spambase.csv\")\n",
    "head(spam_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-reality",
   "metadata": {},
   "source": [
    "The **gbm** package offers an implementation of boosting.  The acronym **gbm** stands for **G**radient **B**oosted **M**achine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages(\"gbm\")\n",
    "library(gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create test and train set\n",
    "\n",
    "set.seed(3375)\n",
    "testsize <- round(0.2 * nrow(spam_data),0)\n",
    "test_rows <- sample(1:nrow(spam_data),testsize)\n",
    "spam_test <- spam_data[test_rows,]\n",
    "spam_train <- spam_data[-test_rows,]\n",
    "\n",
    "nrow(spam_train)\n",
    "nrow(spam_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model to predict if record is spam\n",
    "spam_model_boost_01 <- gbm(is_spam~.,data=spam_train,distribution='bernoulli',n.trees=500,cv.folds=5)\n",
    "summary(spam_model_boost_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal value for M\n",
    "\n",
    "gbm.perf(spam_model_boost_01, method=\"cv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-deadline",
   "metadata": {},
   "source": [
    "## Using the Boosted Model for Prediction\n",
    "\n",
    "Using the test set that we set aside, we will see how the boosted model can be used for prediction.  We are using the optimal value obtained above for number of trees (value of M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob <- predict(spam_model_boost_01,spam_test,n.trees=488,type=\"response\")\n",
    "head(test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-order",
   "metadata": {},
   "source": [
    "### Types of response predictions\n",
    "\n",
    "Like logistic regression, the binary classifier returns log odds of positive response unless **type='response'** is specified. Then the prediction is a probability. It is up to the user to find the best cutoff for this probability to make the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred <- as.integer(test_prob > 0.5)\n",
    "head(test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-aquatic",
   "metadata": {},
   "source": [
    "### Comparing Predictions with Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.frame(Actual=spam_test$is_spam,Probability=test_prob,Predicted=test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-water",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "table(Actual=spam_test$is_spam,Predicted=test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-editing",
   "metadata": {},
   "source": [
    "## Parameters for Fine Tuning gbm Models\n",
    "\n",
    "Ideally, you can try several different values of each parameter to see what yields the best results.\n",
    "\n",
    "The documentation for **gbm** gives more detail on the options (parameters) and on what is stored in the model that is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "?gbm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
